# 머신러닝의 주요 개념

머신러닝을 알기 위해서는 주요 4가지 개념을 이해해야 합니다.

* 모델 : 데이터를 바라보는 시점과 가정
* 손실함수 : 모델의 수식화된 학습 목표
* 최적화 : 손실함수로 표현된 모델을 실제로 학습
* 모델평가 : 모델의 성능이 실제 상황에서 어떨지 추정

## 2.1 모델 : 문제를 바라보는 관점

* 모델은 머신러닝의 시작이라고도 할 수 있습니다. 모델의 정의와 분류, 좋은 모델의 특징에 대해 알아보겠습니다.

### 2.1.1 모델이란

데이터를 갖고 문석을 할 때 패턴이 있다는 믿음을 갖고 시작하게 됩니다. 수학에서는 이것을 가정이라하며, 이런 여러가지 가정을 한데 모은 것을 머신러닝에서는 모델이라 합니다.

일반적으로 머신러닝의 과정은 아래와 같습니다.

`모델정하기 -> 모델 수식화하기 -> 모델 학습하기 -> 모델 평가하기 -> (필요에 따라 처음으로)`

과정1에서 가정을 다음과 같다고 합시다.

```이 데이터에서 x와 y는 선형적인 상관관계를 가진다. 즉, 임의의 w에 대해 y = wx와 같은 관계를 가질 것이다.```

그렇다면 과정3의 가정은 다음과 같을 수 있습니다.

```데이터를 토대로 추측한 결과 y = 2x와 같은 형태를 가진다. 그러므로 w는 2이다.```

모델을 3가지로 분류 해보겠습니다.

* 간단한 모델
* 복잡한 모델
* 구조가 있는 모델

좋은 모델이란 무엇인지, 그리고 좋은 모델을 얻기 위한 `편향-분산 트레이드오프`와 `정규화`에 대해서도 알아보겠습니다.

### 2.1.2 간단한 모델

모델이 간단하다는 것은 데이터가 간단하다는 뜻입니다. 가장 간단하시만 아주 효과적인 모델로는 `선형 모델`이 있습니다. `선형 회귀`는 대표적인 선형 모델입니다.

[선형회귀, 데이터(x)로 학습된 모델(점선), 수식은 y = -5+0.4x]

![그림](images/pic2_2.jpg)

선형회귀 정의는 다음과 같습니다.

> 수식 : Y = W<sub>0</sub> + W<sub>1</sub>X<sub>1</sub> + W<sub>2</sub>X<sub>2</sub> + ...
>
> 출력값(Y)이 입력값(피처)(X<sub>1</sub>,X<sub>2</sub>, ...)에 대해 선형적인 관계

선형관계는 출력값이 입력값(피처값)들에 가중치를 곱한 값의 합(선형결합)으로 표현되는 관계입니다. 선형적인 관계는 예를 들자면 생산량(X)과 불량품 수(Y)를 들 수 있습니다. 생산량과 불량품 수는 일반적으로 양의 상관관계인 경우가 많으므로 선형 회귀 모델이 아주 적합니다.

간단한 모델의 특징

* 데이터가 복잡하지 않고 간단하게 생겼다고 가정
* 결과를 이해하기 쉬움
* 학습이 쉬움
* 가정 자체가 강력해서 모델의 표현능력에 제약이 많음

### 2.1.3 복잡한 모델

복잡한 모델은 데이터가 상대적으로 복잡하게 생겼다고 가정할 때 사용하는 모델입니다. 예를 들어 결정 트리<sup>decision tree</sup>모델을 들 수 있습니다.

![그림](images/pic2_3.jpg)

결정 트리의 정의는 다음과 같습니다.

* 트리의 한 분기점 마다 한가지 조건(보통 입력의 한 부분)을 검사하여 분기를 합니다.
* 모든 분기가 끝나는 리프노드(맨 끝의 노드)에는 결괏값이 들어 있습니다.

결정 트리의 경우 각 데이터마다 식이 포함 될 수 있습니다. 그렇다 보니 전체 데이터에 대한 가정이 적어집니다. 그러나 이 것이 단점이 될 수 있습니다.

복잡한 모델의 특징

* 데이터가 어떻게 생겼을 것이라는 가정 자체가 별로 없습니다.
* 결과를 이해하기 어려울 수도 있습니다.
* 학습이 복잡합니다.
* 한정된 데이터에서만의 변화를 그대로 학습하므로 새로운 데이터에 대해 성능이 떨어질 수 있습니다.

### 2.1.4 구조가 있는 모델

구조가 있는 모델이 위 두가지(간단한 모델, 복잡한 모델)과 별개의 모델이 아니라 어느 항목에도 속할 수 있습니다. 다만, 몇 가지 특정 상황에서 요긴하게 쓰이는 모델이 있어 따로 설명 합니다. 구조가 있는 모델은 입력과 출력의 상관 관계를 학습할 뿐만 아니라 데이터 구조 자체를 모델링 하는 조금 특이한 모델입니다. 여기서는 순차 모델<sup>sequence model</sup>과 그래프 모델<sup>graphical model</sup>에 대해 다루겠습니다.

#### 2.1.4.1 순차 모델

순차 모델은 연속된 관측값이 서로 연관성이 있을 때 주로 사용합니다. 순차 모델의 대표적인 예는 아래 두가지 입니다.

* CRF<sup>conditional random field</sup>, (조건부 랜덤필드 또는 조건부 무작위장)
* RNN<sup>recurrent neural net</sup>, (순환신경망 또는 재귀신경망)

이들 모델의 특징은 특정 시점에서 상태를 저장하고 상태가 각 시점의 입력과 출력에 따라 변화한다는 점입니다.

RNN에 대해 알아봅시다.

![그림](images/pic2_4.png)

RNN의 기본적인 정의는 아래와 같습니다.

* 수식: h<sub>t</sub> = w<sub>0</sub> + w<sub>1</sub>h<sub>t-1</sub> + 
w<sub>2</sub>x<sub>t</sub>
* 실제로 관측되지는 않았지만 특정 시점에 어떤 상태 h<sub>t</sub>가 존재한다고 가정합시다.
* 현재의 상태(h<sub>t</sub>)는 바로 직전의 상태(h<sub>t-1</sub>)와 현재의 입력 데이터 (x<sub>t</sub>)에 영향을 받습니다.
* 상태(h<sub>t</sub>)에 따라 출력(y<sub>t</sub>)이 결정됩니다.

RNN은 상탯값이 다른 모델과의 차이점 입니다.

#### 2.1.4.2 그래프 모델

그래프 모델은 그래프를 이용해서 순차 모델 보다 좀 더 복잡한 구조를 모델링 합니다. 예를 들면 문서의 문법구조를 직접 모델링하거나 이미지의 픽셀 사이의 관계를 네트워크로 보고 그래프로 표현하여 모델링 합니다. 대표적인 마르코프 랜덤 필드<sup>markov random field</sup>(MRF)를 살펴보겠습니다.

![그림](images/pic2_5.jpg)

그림에서 관측된 값(x)은 각 위치에서의 숨겨진 상태(h)에 의해 결정된다고 가정합니다.

* 데이터의 숨겨진 상태(h)사이에 어떤 연결구조가 있다고 가정한 후, 연결된 위치의 숨겨진 상태끼리는 연관성이 있다고 가정합니다.
* 보통 사진을 처리할 때 많이 사용하는데, 이런 경우에는 사진의 특정 위치의 상태(h)가 바로 근접한 위치의 상태들과 관계가 있다고 가정합니다.
* 그리고 실제로 관측된 값(x)은 상태에 따라 결정된다고 가정합니다.

### 2.1.5 좋은 모델이란 무엇인가

좋은 모델이란 `데이터의 패턴을 잘 학습한 모델` 이라고 할 수 있습니다. 모델을 평가하는 많은 이론이 있지만 그중 모델의 복잡도와 표현혁에 대한 균형을 다루는 `편향-분산 트레이드 오프<sup>bias-variance trade-off</sup>와 균형을 자동으로 학습하게 하는 정규화<sup>regularization</sup>에 대해 알아보겠습니다.

#### 2.1.5.1 편향-분산 트레이드오프

편향과 분산에 대해서는 [통계의 기초](https://github.com/swkwon/study/blob/master/math/통계의기초.md) 에서 설명을 하였습니다. 즉, 에러를 줄이기 위해서는 편향을 줄이거나 분산을 줄여야 합니다.

![그림](images/pic2_6.png)

위 그림처럼 편향과 분산을 적당히 줄이면 어느정도 적당한 결과를 찾을 수 있게 됩니다. 편향이나 분산을 줄이는 대표적인 예로 다음과 같은 방법이 있습니다.

* 부스팅<sup>boosting</sup> : 간단한 모델을 여러개 조합하여 편향을 줄이는 방법
* 랜덤 포레스트<sup>random forest</sup> : 복잡한 모델인 결정 트리를 여러개 조합하여 분산을 줄이는 방법

#### 2.1.5.2 정규화

정규화는 정해진 모델이 필요 이상으로 복잡해지지 않도록 조절하는 트릭입니다. 모델이 데이터에 비해 필요 이상으로 복잡하면 불필요한 노이즈까지 학습해서 학습할 때는 성능이 좋지만, 실제로 사용할 때는 좋지 않은 성능을 보일 수 있습니다.모델의 복잡도를 줄이는 방법으로 크게 2가지가 있습니다.

* 모델 변경 : 데이터를 표현하는 방법을 완전히 새롭게 전환해서 적합한 모델을 찾는 방법(모델선택 방법은 `편향-분산 트레이드오프` 참조)
* 정규화 : 모델에 들어 있는 인자에 제한을 주어 모델이 필요 이상으로 복잡해지지 않게 하는 방법

구체적인 예를 들어 봅시다.

![그림](images/pic2_2.jpg)

위의 그림처럼 선형 회귀를 한다고 생각합시다. 입력은 3개 x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub> 고 출력은 1개(y)라고 가정합시다. 이때 선형 회귀 모델은 다음과 같습니다.

> y = w<sub>0</sub> + w<sub>1</sub>x<sub>1</sub> + w<sub>2</sub>x<sub>2</sub> + w<sub>3</sub>x<sub>3</sub>

이 모델에서 2가지 입력을 무시하고 더 간단한 모델로 바꿔보겠습니다.

> y = w<sub>0</sub> + w<sub>1</sub>x<sub>1</sub>

또는 더 복잡한 연관 관계를 이용하는 모델을 다은과 같이 만들 수도 있습니다.

> y = w<sub>0</sub> + w<sub>1</sub>x<sub>1</sub> + w<sub>2</sub>x<sub>2</sub> + w<sub>3</sub>x<sub>3</sub> + w<sub>4</sub>x<sub>1</sub>x<sub>2</sub> + w<sub>5</sub>x<sub>2</sub>x<sub>3</sub> + w<sub>6</sub>x<sub>1</sub>x<sub>3</sub>

정규화 모델은 표현식에 추가적으로 제약 조건을 걸어서 모델이 필요 이상으로 복잡해지지 않도록 자동으로 조정해주는 기법입니다. 복잡한 식에 추가적인 제약 조건을 도입해서 필요 없는 인자들을 제거하게 됩니다.

> y = w<sub>0</sub> + w<sub>1</sub>x<sub>1</sub> + w<sub>2</sub>x<sub>2</sub> + w<sub>3</sub>x<sub>3</sub> (단, w<sub>1</sub>, w<sub>2</sub>, w<sub>3</sub>중에 두개만 0이 아닌 값을 가진다)

이렇게 되면 위 식은 3가지 식으로 표현할 수 있습니다.

> y = w<sub>0</sub> + w<sub>1</sub>x<sub>1</sub> + w<sub>2</sub>x<sub>2</sub>
>
> y = w<sub>0</sub> + w<sub>2</sub>x<sub>2</sub> + w<sub>3</sub>x<sub>3</sub>
>
> y = w<sub>0</sub> + w<sub>1</sub>x<sub>1</sub> + w<sub>3</sub>x<sub>3</sub>

이렇게 되면 모델은 위 3가지 식중 가장 적합한 하나를 자동으로 찾아줍니다.

## 2.2 손실함수 : 모델의 수식화된 학습목표

손실함수<sup>loss function</sup> : 모델이 실제 데이터를 바르게 표현했는지 혹은 얼마나 예측이 정확한지 평가하는 함수

손실함수로 얻은 결괏값은 `에러`라고 부르며 작을 수록 모델이 정확하게 학습된 것입니다.

손실함수는 데이터 전체에 대해 계산하는 함수지만, 간단하게 표시하기 위해 한 개의 데이터에 대해서만 정의하기도 합니다. 이때 각 데이터에 대한 손실함수 계산 결과의 총합이 그 모델과 데이터 전체에 대한 손실함수 결과 입니다. 이런 경우에는 몇가지 가정이 추가됩니다. 예를 들어 데이터셋에서 각각의 데이터가 서로 확률적 독립이고 같은 분포를 가진다는 i.i.d 가정(독립항등분포)이 대표적입니다.

* 산술 손실함수 : 모델로 산술값을 예측할 때 데이터에 대한 예측값과 실제 관측값을 비교하는 함수 입니다. 주로 회귀 문제에서 사용합니다.
* 확률 손실함수 : 모델로 항목이나 값에 대한 확률을 예측하는 경우에 사용합니다. 매우 유연하기 때문에 회귀 문제를 비롯해 보편적으로 사용합니다.
* 랭킹 손실함수 : 모델로 순서를 결정할 때 사용합니다. 추천 시스템에 주로 사용합니다.
* 모델 복잡도와 관련된 손실함수 : 보통 위 손실함수들과 합쳐져서 모델이 필요 이상으로 복잡해지지 않도록 방지하는 손실함수 입니다. 앞에서 설명한 정규화의 일종입니다.

우리는 위 4가지 손실함수에 대해 다룰 것입니다.

### 2.2.1 산술 손실함수

모델로 산술값을 예측할 때 각 데이터에 대한 예측값과 실제 관측값의 차이를 산술적으로 계산하는 손실 함수를 많이 사용합니다. 차이의 제곱을 사용하는 `제곱 손실함수`와 차이의 절댓값을 사용하는 `손실함수`가 많이 쓰입니다.

제곱 손실함수를 쓰는 이유를 간단하게 설명하자면 주어진 데이터 출력값(y)과 모델의 예측값 (ŷ) 의 차이의 제곱을 계산하므로, 최적화가 쉽고, 손실값의 이해가 쉽기 때문입니다. 제곱 손실함수는 다음과 같습니다.

loss(f) = (y - ŷ)<sup>2</sup>

f : 모델
y : 데이터로부터 주어진 출력
ŷ : 모델에 데이터로부터 주어진 입력을 넣어서 계산한 값

예를들어 어떤 선형 모델을 가정할 때, 입력값 x가 모델의 추정치 ŷ와 다음과 같은 관계를 가집니다.

> 선형모델 f : x -> ŷ = w<sub>0</sub> + w<sub>1</sub>x

이 경우 제곱 손실함수는 아래와 같이 정의 할 수 있습니다.

> 선형모델 loss(f) = (y - w<sub>0</sub> - w<sub>1</sub>x)<sup>2</sup>

위 예는 입력 x와 출력 y인 데이터 1개에 대해 학습된 모델이 얼마만큼의 에러를 가지는지, 즉 얼마나 데이터를 잘 표현하는지 정량적으로 계산합니다. 전체 데이터에 대해서는 모든 손실함수의 값을 합산하거나 평균 값을 사용합니다. 평균의 경우 `평균 제곱 편차`라는 표현을 주로 사용합니다.

예를 들어 보겠습니다.

x = 1, y = 3 일 때 손실함수를 계한하겠습니다. 일단 학습을 하기전 w<sub>0</sub>과 w<sub>1</sub>을 0으로 초기화 합니다.

> (3 - 0 - 0 x 1)<sup>2</sup> = 9

여기서는 설명하지 않았지만 우연히 w<sub>1</sub> = 2 일 때 더 좋을 것이라 추측했다고 합시다. 그 경우 손실함수는 다음과 같이 계산 됩니다.

> (3 - 0 - 2 x 1)<sup>2</sup> = 1

w<sub>0</sub>과 w<sub>1</sub>이 0 일 때 보다 더 작은 값이 나왔습니다. 더 작은 에러가 나왔다는 것은 더 정확하게 학습된 모델이라는 뜻입니다. 만일 w<sub>0</sub> = 1을 넣으면 에러는 0이 되고 해당 데이터에 대해 완벽한 모델이 됩니다. (그렇지만 실제로는 발생하기 힘듭니다.)

### 2.2.2 확률 손실함수

위의 산술 손실함수는 산술값을 예측하기 위한 회귀 모델에 적합합니다. 확률 손실함수는 특정항목을 고르는 분류모델에 더 적합합니다.