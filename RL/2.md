# 2 밴딧 문제

멀티암드 밴딧(multi-armed bandit)은 강화학습에서 가장 단순한 형태의 문제입니다. 문제는 n개의 손잡이(arm)가 달린 밴딧(bandit)이라는 뜻입니다.

에이전트의 목표는 시간의 흐름에 따라 가장 높은 보상을 제공하는 손잡이를 찾아내고 항상 이 손잡이를 선택함으로써 돌아오는 보상을 최대화 하는 것입니다.

여기서 간단하게 강화학습의 세가지 요소를 살펴보겠습니다.

1. 액션 의존성 : 각 액션은 다른 보상을 가져옵니다.
1. 시간 의존성 : 보상은 시간이 지연되고 나서야 주어 집니다. 예를 들어 미로 탐험 시 어느 분기점에서 왼쪽길을 선택했다면 이 선택이 옳았는지는 미로를 빠져나가야 알게 된다는 것입니다.
1. 상태 의존성 : 어떤 액션에 대한 보상은 환경의 상태에 좌우됩니다.

강화학습에서 밴딧 문제가 가장 좋은 시작점이 되는 이유는 위 세가지 요소 중 1번만 고려하면 됩니다.

손잡이가 n개인 슬롯머신에서 고려해야 하는 것은, 어떤 보상이 어떤 액션과 연관되어 있는지, 그리고 우리가 최적의 액션을 선택하도록 보장하는 것이 전부 입니다. 강화학습에서는 이를 정책(policy)이라고 합니다.

밴딧 문제를 해결하도록 에이전트를 학습시키기 위해 `정책 경사 방법`을 단순화 하여 사용하겠습니다.

## 2.1 정책 경사

정책 경사(policy gradient) 네트워크를 가장 단순하게 표현하려면, 분명한 출력값을 산출하는 네트워크를 떠올리면 됩니다.밴딧의 네트워크는 단지 일련의 가중치로 구성되게 되는데, 각각의 가중치는 밴딧에서 당기는 각각의 손잡이에 매치됩니다. 각 가중치는 에이전트가 생각하기에 해당 손잡이를 당기는 것이 얼마나 좋은지를 나타내며, 모든 가중치를 1로 초기화하면 에이전트가 모든 손잡이의 잠재적 보상에 대해 어느정도 낙관적으로 바라본다는 의미가 됩니다.

네트워크를 업데이트 하기 위해, 단순히 e의 확률로 랜덤한 액션을 취하는 그리디(greedy) 정책을 가지는 손잡이 하나를 떠올려보겠습니다. 일단 에이전트가 액션을 취하게 되면 1 또는 -1의 보상을 받습니다. 이 보상을 `정책 비용 방정식`에 적용해 네트워크를 업데이트 할 수 있게 됩니다. 수식은 아래와 같습니다.

> Loss = -log(π) * A

A는 어드밴티지(advantage)로서 모든 강화학습 알고리즘에 필수적인 요소입니다. A는 액션이 어떤 기준선보다 얼마나 더 나은지의 정도를 의미합니다. π는 정책입니다. 현재 우리의 예제에서 정책은 곧 선택된 액션의 가중치에 해당합니다. 우리는 위 비용함수(loss function)를 통해 (+)의 보상을 가져오는 액션에 대해서는 가중치를 증가시키고 (-)의 보상을 가져오는 액션에 대해서는 가중치를 감소시킬 수 있을 것입니다.

## 2.2 멀티암드 밴딧의 구현

멀티암드 밴딧 문제를 풀 수 있는 정책 경사 기반의 에이전트를 구현하는 방법에 대해 간단한 예제를 살펴보겠습니다.

[쥬피터 소스코드](https://github.com/awjuliani/DeepRL-Agents/blob/master/Simple-Policy.ipynb)

[머신러닝 테스트 사이트](colab.research.google.com)